---
theme: slidev-theme-nearform
background: https://images.unsplash.com/photo-1636690581110-a512fed05fd3?q=80&w=1920&auto=format&fit=crop
title: Introducci√≥n Pr√°ctica a LLMs con Ollama
info: Workshop por EmaSuriano
transition: slide-left
---

# Introducci√≥n Pr√°ctica a LLMs con Ollama

## Workshop por [EmaSuriano](https://emasuriano.com)

<div class="abs-br m-6 text-xl">
  <a href="https://github.com/EmaSuriano/workshop-ollama" target="_blank" class="slidev-icon-btn">
    <carbon:logo-github />
  </a>
</div>

---

# ¬øQu√© haremos hoy?

- Entender qu√© son los LLMs locales y por qu√© son importantes
- Instalar y configurar Ollama en tu computadora
- Experimentar con diferentes modelos de lenguaje
- Crear interfaces gr√°ficas para interactuar con tus modelos
- Desarrollar aplicaciones pr√°cticas con IA local

---

# ¬øPor qu√© LLMs Locales?

- **Privacidad**: Tus datos nunca salen de tu computadora
- **Sin conexi√≥n**: Funcionan sin necesidad de internet
- **Personalizaci√≥n**: Control total sobre los modelos y par√°metros
- **Econom√≠a**: Sin costos recurrentes por llamadas a APIs
- **Latencia**: Respuestas m√°s r√°pidas sin depender de servidores externos

---

# Comparativa: Local vs Cloud

<v-clicks>

| Caracter√≠stica | LLMs Locales | LLMs en la Nube |
|----------------|--------------|-----------------|
| Privacidad     | ‚úÖ Total     | ‚ùå Limitada     |
| Potencia       | ‚ö†Ô∏è Limitada por hardware | ‚úÖ Alta |
| Costo          | ‚úÖ Gratis/√önico | ‚ùå Suscripci√≥n |
| Personalizaci√≥n| ‚úÖ Completa   | ‚ö†Ô∏è Limitada    |
| Facilidad      | ‚ö†Ô∏è M√°s t√©cnico | ‚úÖ Sencillo    |
| Disponibilidad | ‚úÖ 24/7 Offline | ‚ùå Requiere internet |

</v-clicks>

---
layout: two-cols-header
---

# Requisitos T√©cnicos

::left::

## M√≠nimos:

- CPU moderno (4+ cores)
- 8GB RAM
- 10GB espacio libre
- Sistema operativo compatible

::right::

## Recomendados:

- GPU NVIDIA (CUDA)
- 16GB+ RAM
- 50GB+ espacio libre

---
layout: image-right
image: ./assets/ollama.png
---

# **Ollama**: Tu Plataforma de LLMs Locales

- Framework open source para ejecutar LLMs localmente
- Soporte para m√∫ltiples modelos (Llama, Mistral, Phi, etc.)
- API simple y potente
- Interfaz de l√≠nea de comandos intuitiva

---
layout: image-right
image: ./assets/ollama-install.png
---

# Instalaci√≥n de Ollama


Ollama tiene soporte para todos los Sistemas Operativos: 

* [Mac](https://ollama.com/download/mac)
* [Linux](https://ollama.com/download/linux)
* [Windows](https://ollama.com/download/windows)


---

# Primeros Pasos con Ollama

Ollama en si no tiene una interfaz, por lo que la forma de comunicarnos es usando la consola: 

```bash
> ollama

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
```

---

# Workflow normal de trabajo con modelos 

<v-clicks every="2">

## 1. Descargar un modelo

```bash
> ollama pull llama3.2
```

## 2. Iniciar una conversaci√≥n
```bash
> ollama run llama3.2
>>> Hi
How can I help you today?
```

## 3. Consultar desde la terminal

```bash
> ollama run llama3.2 "Contame un chiste bueno"
¬°Claro! Aqu√≠ tienes uno:
¬øPor qu√© la computadora fue al doctor?
¬°Porque ten√≠a un virus!
Espero que te haya hecho sonre√≠r. ¬øQuieres escuchar otro?
```

</v-clicks >

---
layout: image-right
image: ./assets/ollama-models.png
---

# Modelos Populares en Ollama

- **Mistral** - Balanceado en tama√±o y rendimiento
- **Llama** - Varias versiones y tama√±os disponibles
- **Phi** - Compacto pero potente
- **Gemma** - El modelo abierto de Google
- **Deepseek-r1** - Modelo chino con razonamiento

---
layout: image
image: https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExbWRwbzU0b2MweDBqdWFjZmU0NzVuMHlveTdiM3BtYWpvOHc0eGJqcSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/fxqt51CAMGITJlxcRI/giphy.gif
---

<!-- Demo Ollama -->

---
layout: quote
---

# "Todo muy lindo, pero esto no se parece a Chat GPT"

-. Dicho por todo el mundo ...

---
layout: iframe-right
url: https://msty.app/
---

# MSTY 

Es la forma m√°s sencilla de usar modelos de IA locales y en l√≠nea con integraci√≥n perfecta de Ollama y sin necesidad de configuraci√≥n t√©cnica.

## Features

- Sin configuraci√≥n t√©cnica (sin Docker, sin terminal)
- Compara m√∫ltiples respuestas de IA lado a lado
- Importa archivos y conecta fuentes de conocimiento
- Mejor UX para Ollama

---
layout: iframe
url: https://docs.msty.app/getting-started/download
---

# MSTY: Instalaci√≥n

---
layout: image
image: https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExcm5tdnp0dGt1anU2Z2ZwcXBvejhreW42cDZsNm8wODNxOTdpYXJwYSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/WcpaItX5JHYkw/giphy.gif
---

---
layout: quote
---

# "Todo muy lindo, pero como hago una aplicaci√≥n?"

-. Dicho por los devs ...

---

```mermaid {scale:0.7}
flowchart TD
    %% Clients
    WebApp([Web Application])
    MobileApp([Mobile Application])
    DesktopApp([Desktop Application])
    CLI([Command Line Tool])
    
    %% Backend components
    subgraph Backend["Backend Services"]
        direction TB
        Gateway[API Gateway]
        
        subgraph AppLayer["Application Layer"]
            direction LR
            Logic[Business Logic]
            DB[(Database)]
        end
        
        subgraph AILayer["AI Layer"]
            direction LR
            Framework["LLM Framework"]
            OllamaService["Ollama"]
        end
    end
    
    %% Client connections
    WebApp -->|HTTP/REST| Gateway
    MobileApp -->|HTTP/REST| Gateway
    DesktopApp -->|HTTP/REST| Gateway
    CLI -->|HTTP/REST| Gateway
    
    %% Backend connections
    Gateway <--> Logic
    Logic <--> DB
    Logic <-->|Requests models| Framework
    Framework <-->|API calls| OllamaService
    
    %% Visual styling
    classDef clients stroke:#1890ff
    classDef gateway stroke:#52c41a
    classDef appLayer stroke:#d48806
    classDef aiLayer stroke:#fa541c
    classDef database stroke:#722ed1
    
    class WebApp,MobileApp,DesktopApp,CLI clients
    class Gateway gateway
    class Logic appLayer
    class Framework,OllamaService aiLayer
    class DB database
    
    %% Add titles
    subgraph Clients["Client Applications"]
        WebApp
        MobileApp
        DesktopApp
        CLI
    end
```

---
layout: two-cols-header
---

# Stack de una aplicaci√≥n (real)

::left::

<v-click>

## Frontend

* Web Application: ReactJS
* Mobile Application: Kotlin / Swift
* Desktop Application: Electron
* Command Line Tool: Commander

</v-click>

::right::

<v-click>

## Backend

* API Gateway: FastAPI, Flask, etc.
* Application Layer + Database
* AI Layer: üéÅ
* Ollama: ya lo vimos 

</v-click>

---

# Stack de una aplicaci√≥n (workshop)

## Frontend + Backend

* Frontend: Chainlit / Streamlit ‚ú®
* Application Layer + Database
* AI Layer: üéÅ
* Ollama: ya lo vimos 


---

# Streamlit

Streamlit es un framework de Python para construir r√°pidamente aplicaciones web interactivas para ciencia de datos e inteligencia artificial.

- **Simplicidad**: Crea aplicaciones web con Python puro (sin necesidad de HTML/CSS/JS)
- **Prototipado r√°pido**: Convierte scripts de datos en aplicaciones web compartibles en minutos
- **Elementos interactivos**: Widgets incorporados para entradas de usuario (deslizadores, botones, entradas de texto)
- **Reactividad**: Actualizaciones autom√°ticas cuando cambian las entradas
- **Visualizaci√≥n de datos**: Integraci√≥n perfecta con bibliotecas populares de gr√°ficos
- **Gesti√≥n de estado**: Estado de sesi√≥n para mantener el estado de la aplicaci√≥n entre ejecuciones
- **Opciones de despliegue**: F√°cil implementaci√≥n a trav√©s de Streamlit Cloud u otros servicios

---
layout: full
---

<video controls autoplay loop src="https://s3-us-west-2.amazonaws.com/assets.streamlit.io/videos/hero-video.mp4" />

---

# Chainlit

Chainlit es un framework de Python dise√±ado espec√≠ficamente para construir aplicaciones de IA conversacional.

- **Enfocado en aplicaciones LLM**: Optimizado para construir chatbots e interfaces conversacionales
- **Gesti√≥n de conversaciones**: Caracter√≠sticas incorporadas para manejar contextos e historiales de chat
- **Componentes de UI**: Elementos prefabricados para interfaces de chat
- **Transmisi√≥n de mensajes**: Soporte para transmitir respuestas desde modelos de lenguaje
- **Multi-modal**: Maneja texto, im√°genes y archivos adjuntos en conversaciones
- **Integraci√≥n con bibliotecas LLM**: Funciona bien con LangChain, LlamaIndex y otros frameworks de LLM
- **Herramientas de depuraci√≥n**: Herramientas para ayudar a rastrear y depurar la ejecuci√≥n de aplicaciones LLM
- **Despliegue en la nube**: Opciones de implementaci√≥n similares a Streamlit

---
layout: full
---

<video controls autoplay loop src="https://mintlify.s3.us-west-1.amazonaws.com/chainlit-43/images/overview.mp4" />

---

# Tabla de Decisi√≥n


| **Necesidad** | **Mejor Opci√≥n** |
|---------------|------------------|
| Aplicaci√≥n de chat o conversacional | Chainlit |
| Visualizaci√≥n de datos | Streamlit |
| Tableros interactivos | Streamlit |
| Interfaces para modelos ML | Streamlit |
| Integraci√≥n avanzada con LLMs | Chainlit |
| Mayor comunidad y recursos | Streamlit |
| Interfaces optimizadas para chat | Chainlit |


---

# Gu√≠a R√°pida

##  **Streamlit** para: aplicaciones de datos, visualizaciones y dashboards
##  **Chainlit** para: chatbots e interfaces conversacionales

---
layout: section
---

# asdasd


---
layout: image-right
image: ./assets/chainlit.png
---

# Chainlit

asdasd


---
layout: full
---

<video controls>
  <source src="https://mintlify.s3.us-west-1.amazonaws.com/chainlit-43/images/overview.mp4" type="video/mp4">
</video>


---

# Interfaces de Usuario: Streamlit

![Streamlit Logo](https://streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png)

- Framework Python para crear aplicaciones web interactivas
- Perfecto para prototipos r√°pidos
- Componentes interactivos integrados
- F√°cil visualizaci√≥n de datos

---

# Ejemplo B√°sico con Streamlit

```python
import streamlit as st
from ollama import Client

st.title("Mi Asistente con LLM Local")

# Inicializar cliente de Ollama
client = Client(host="http://localhost:11434")

# Interfaz de usuario
prompt = st.text_input("Tu pregunta:", 
                     placeholder="¬øQu√© quieres saber?")

if st.button("Generar respuesta"):
    with st.spinner("Pensando..."):
        response = client.chat(model="mistral", 
                            messages=[{"role": "user", "content": prompt}])
        st.write(response["message"]["content"])
```

---

# Componentes Interactivos de Streamlit

- **Entradas**: Text inputs, sliders, selectors
- **Visualizaci√≥n**: Charts, tables, maps
- **Media**: Images, audio, video
- **Layouts**: Columns, tabs, expandable sections
- **Estado**: Session state para persistencia
- **Deployment**: F√°cil de implementar en la nube

---

# Demo: Interfaz Avanzada

- Selecci√≥n de modelos
- Historial de chat persistente
- Configuraci√≥n de par√°metros (temperatura, tokens)
- Exportaci√≥n de resultados
- Visualizaci√≥n de latencia y tokens

---

# Agentic AI con phidata

![Phidata Logo](https://docs.phidata.com/img/phidata-logo.png)

- Framework para construir agentes AI aut√≥nomos
- Integraci√≥n directa con Ollama
- Workflows inteligentes
- Personalizable y extensible

---

# Creaci√≥n de un Agente B√°sico

```python
from phi.assistant import Assistant

# Crear un asistente con modelo local
assistant = Assistant(
    name="Asistente Python",
    llm="ollama/codellama",
    description="Experto en Python y resoluci√≥n de problemas",
    instructions="""
    - Ayuda con c√≥digo Python
    - Explica conceptos de programaci√≥n
    - Sugiere buenas pr√°cticas
    """
)

# Interactuar con el asistente
response = assistant.run("Escribe una funci√≥n que calcule fibonacci")
print(response)
```

---

# Funcionalidades de los Agentes

- **Memoria**: Recordar conversaciones previas
- **Herramientas**: Acceso a APIs, bases de datos, b√∫squeda
- **Workflows**: Secuencias autom√°ticas de tareas
- **Retroalimentaci√≥n**: Aprendizaje por refuerzo
- **Multi-agente**: Colaboraci√≥n entre agentes

---

# Desarrollo de APIs con FastAPI

![FastAPI Logo](https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png)

- Framework moderno para APIs en Python
- Alto rendimiento con Async/Await
- Documentaci√≥n autom√°tica con Swagger
- Validaci√≥n de datos integrada
- F√°cil integraci√≥n con Ollama

---

# Ejemplo de API con FastAPI

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from ollama import Client

app = FastAPI()
client = Client()

class Query(BaseModel):
    prompt: str
    model: str = "mistral"
    temperature: float = 0.7

@app.post("/generate")
async def generate_response(query: Query):
    try:
        response = client.chat(
            model=query.model,
            messages=[{"role": "user", "content": query.prompt}],
            temperature=query.temperature
        )
        return {"response": response["message"]["content"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

# Integraci√≥n Frontend-Backend

- **React/Vue/Angular**: Frontend moderno
- **WebSockets**: Para streaming de respuestas
- **Gesti√≥n de estado**: Control de sesiones y contexto
- **Responsivo**: Adaptable a m√≥viles y desktop
- **Microservicios**: Arquitectura escalable

---

# Mejores Pr√°cticas

- **Optimizaci√≥n de prompts**: Instrucciones claras y espec√≠ficas
- **Gesti√≥n de contexto**: Limitar tama√±o para rendimiento
- **Caching**: Almacenar respuestas comunes
- **Monitoreo**: Tracking de uso y rendimiento
- **Fallbacks**: Plan B cuando el modelo falla
- **Feedback loop**: Mejora continua basada en uso real

---

# Casos de Uso Pr√°cticos

- **Asistente de programaci√≥n personal**
- **An√°lisis de documentos locales**
- **Generaci√≥n de contenido sin conexi√≥n**
- **Chatbots personalizados para equipos**
- **Automatizaci√≥n de tareas repetitivas**
- **Educaci√≥n y entrenamiento interactivo**

---

# Recursos Adicionales

- [Documentaci√≥n de Ollama](https://github.com/ollama/ollama)
- [Streamlit Tutorial](https://docs.streamlit.io/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Phidata Agents Guide](https://docs.phidata.com/)
- [Hugging Face - Modelos Open Source](https://huggingface.co/)
- [Comunidad Discord Workshop](https://discord.gg/workshop)

---

# Actividad Pr√°ctica

1. Instala Ollama y descarga un modelo
2. Crea una interfaz simple con Streamlit
3. Implementa una funcionalidad espec√≠fica
4. Presenta tu proyecto al grupo

¬°Tienes 45 minutos!

---

# ¬°Gracias por Participar!

## Contacto:
- Email: workshop@example.com
- Twitter: @workshop_ai
- GitHub: github.com/workshop-ai

## Pr√≥ximos Eventos:
- Embeddings y recuperaci√≥n de informaci√≥n
- Fine-tuning de modelos locales
- Deployment en producci√≥n

---
layout: fact
---

# Q&A